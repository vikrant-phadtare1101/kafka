
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-->

<h3><a id="security_overview" href="#security_overview">7.1 Security Overview</a></h3>
In release 0.9.0.0, the Kafka community added a number of features that, used either separately or together, increases security in a Kafka cluster. The following security measures are currently supported:
<ol>
    <li>Authenticating clients (Producers and consumers) connections to brokers, using either SSL or SASL (Kerberos)</li>
    <li>Authorizing read / write operations by clients</li>
    <li>Encryption of data sent between brokers and clients, or between brokers, using SSL (Note there is performance degradation in the clients when SSL is enabled. The magnitude of the degradation depends on the CPU type.)</li>
    <li>Authenticate brokers connecting to ZooKeeper</li>
    <li>Security is optional - non-secured clusters are supported, as well as a mix of authenticated, unauthenticated, encrypted and non-encrypted clients.</li>
    <li>Authorization is pluggable and supports integration with external authorization services</li>
</ol>

The guides below explain how to configure and use the security features in both clients and brokers.

<h3><a id="security_ssl" href="#security_ssl">7.2 Encryption and Authentication using SSL</a></h3>
Apache Kafka allows clients to connect over SSL. By default SSL is disabled but can be turned on as needed.

<ol>
    <li><h4><a id="security_ssl_key" href="#security_ssl_key">Generate SSL key and certificate for each Kafka broker</a></h4>
        The first step of deploying HTTPS is to generate the key and the certificate for each machine in the cluster. You can use Java’s keytool utility to accomplish this task.
        We will generate the key into a temporary keystore initially so that we can export and sign it later with CA.
        <pre>$ keytool -keystore server.keystore.jks -alias localhost -validity {validity} -genkey</pre>

        You need to specify two parameters in the above command:
        <ol>
            <li>keystore: the keystore file that stores the certificate. The keystore file contains the private key of the certificate; therefore, it needs to be kept safely.</li>
            <li>validity: the valid time of the certificate in days.</li>
        </ol>
        Ensure that common name (CN) matches exactly with the fully qualified domain name (FQDN) of the server. The client compares the CN with the DNS domain name to ensure that it is indeed connecting to the desired server, not the malicious one.</li>

    <li><h4><a id="security_ssl_ca" href="#security_ssl_ca">Creating your own CA</a></h4>
        After the first step, each machine in the cluster has a public-private key pair, and a certificate to identify the machine. The certificate, however, is unsigned, which means that an attacker can create such a certificate to pretend to be any machine.<p>
        Therefore, it is important to prevent forged certificates by signing them for each machine in the cluster. A certificate authority (CA) is responsible for signing certificates. CA works likes a government that issues passports—the government stamps (signs) each passport so that the passport becomes difficult to forge. Other governments verify the stamps to ensure the passport is authentic. Similarly, the CA signs the certificates, and the cryptography guarantees that a signed certificate is computationally difficult to forge. Thus, as long as the CA is a genuine and trusted authority, the clients have high assurance that they are connecting to the authentic machines.
        <pre>openssl req <b>-new</b> -x509 -keyout ca-key -out ca-cert -days 365</pre>

        The generated CA is simply a public-private key pair and certificate, and it is intended to sign other certificates.<br>

        The next step is to add the generated CA to the **clients’ truststore** so that the clients can trust this CA:
        <pre>keytool -keystore server.truststore.jks -alias CARoot <b>-import</b> -file ca-cert</pre>

        <b>Note:</b> If you configure Kafka brokers to require client authentication by setting ssl.client.auth to be "requested" or "required" on <a href="#config_broker">Kafka broker config</a> then you must provide a truststore for Kafka broker as well and it should have all the CA certificates that clients keys signed by.
        <pre>keytool -keystore client.truststore.jks -alias CARoot -import -file ca-cert</pre>

        In contrast to the keystore in step 1 that stores each machine’s own identity, the truststore of a client stores all the certificates that the client should trust. Importing a certificate into one’s truststore also means that trusting all certificates that are signed by that certificate. As the analogy above, trusting the government (CA) also means that trusting all passports (certificates) that it has issued. This attribute is called the chains of trust, and it is particularly useful when deploying SSL on a large Kafka cluster. You can sign all certificates in the cluster with a single CA, and have all machines share the same truststore that trusts the CA. That way all machines can authenticate all other machines.</li>

    <li><h4><a id="security_ssl_signing" href="#security_ssl_signing">Signing the certificate</a></h4>
        The next step is to sign all certificates generated by step 1 with the CA generated in step 2. First, you need to export the certificate from the keystore:
        <pre>keytool -keystore server.keystore.jks -alias localhost -certreq -file cert-file</pre>

        Then sign it with the CA:
        <pre>openssl x509 -req -CA ca-cert -CAkey ca-key -in cert-file -out cert-signed -days {validity} -CAcreateserial -passin pass:{ca-password}</pre>

        Finally, you need to import both the certificate of the CA and the signed certificate into the keystore:
        <pre>
            $ keytool -keystore server.keystore.jks -alias CARoot -import -file ca-cert
            $ keytool -keystore server.keystore.jks -alias localhost -import -file cert-signed
        </pre>

        The definitions of the parameters are the following:
        <ol>
            <li>keystore: the location of the keystore</li>
            <li>ca-cert: the certificate of the CA</li>
            <li>ca-key: the private key of the CA</li>
            <li>ca-password: the passphrase of the CA</li>
            <li>cert-file: the exported, unsigned certificate of the server</li>
            <li>cert-signed: the signed certificate of the server</li>
        </ol>

        Here is an example of a bash script with all above steps. Note that one of the commands assumes a password of `test1234`, so either use that password or edit the command before running it.
            <pre>
        #!/bin/bash
        #Step 1
        keytool -keystore server.keystore.jks -alias localhost -validity 365 -genkey
        #Step 2
        openssl req -new -x509 -keyout ca-key -out ca-cert -days 365
        keytool -keystore server.truststore.jks -alias CARoot -import -file ca-cert
        keytool -keystore client.truststore.jks -alias CARoot -import -file ca-cert
        #Step 3
        keytool -keystore server.keystore.jks -alias localhost -certreq -file cert-file
        openssl x509 -req -CA ca-cert -CAkey ca-key -in cert-file -out cert-signed -days 365 -CAcreateserial -passin pass:test1234
        keytool -keystore server.keystore.jks -alias CARoot -import -file ca-cert
        keytool -keystore server.keystore.jks -alias localhost -import -file cert-signed
                </pre></li>
    <li><h4><a id="security_configbroker" href="#security_configbroker">Configuring Kafka Broker</a></h4>
        Kafka Broker comes with the feature of listening on multiple ports thanks to [KAFKA-1809](https://issues.apache.org/jira/browse/KAFKA-1809).
        We need to configure the following property in server.properties, which must have one or more comma-separated values:
        <pre>listeners</pre>

        If SSL is not enabled for inter-broker communication (see below for how to enable it), both PLAINTEXT and SSL ports will be necessary.
        <pre>listeners=PLAINTEXT://host.name:port,SSL://host.name:port</pre>

        Following SSL configs are needed on the broker side
        <pre>
        ssl.keystore.location = /var/private/ssl/kafka.server.keystore.jks
        ssl.keystore.password = test1234
        ssl.key.password = test1234
        ssl.truststore.location = /var/private/ssl/kafka.server.truststore.jks
        ssl.truststore.password = test1234
        </pre>

        Optional settings that are worth considering:
        <ol>
            <li>ssl.client.auth = none ("required" => client authentication is required, "requested" => client authentication is requested and client without certs can still connect when this option chosen")</li>
            <li>ssl.cipher.suites = A cipher suite is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol. (Default is an empty list)</li>
            <li>ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1 (list out the SSL protocols that you are going to accept from clients. Do note SSL is deprecated and using that in production is not recommended)</li>
            <li> ssl.keystore.type = JKS</li>
            <li>ssl.truststore.type = JKS</li>
        </ol>
        If you want to enable SSL for inter-broker communication, add the following to the broker properties file (it defaults to PLAINTEXT)
        <pre>security.inter.broker.protocol = SSL</pre>

        If you want to enable any cipher suites other than the defaults that comes with JVM like the ones listed here:
        <a href="https://docs.oracle.com/javase/7/docs/technotes/guides/security/SunProviders.html">https://docs.oracle.com/javase/7/docs/technotes/guides/security/SunProviders.html</a> you will need to install <b><a href="http://www.oracle.com/technetwork/java/javase/downloads/jce-7-download-432124.html">Unlimited Strength Policy files</a></b><br>

        Once you start the broker you should be able to see in the server.log
        <pre>with addresses: PLAINTEXT -> EndPoint(192.168.64.1,9092,PLAINTEXT),SSL -> EndPoint(192.168.64.1,9093,SSL)</pre>

        To check quickly if  the server keystore and truststore are setup properly you can run the following command
        <pre>openssl s_client -debug -connect localhost:9093 -tls1</pre> (Note: TLSv1 should be listed under ssl.enabled.protocols)<br>
        In the output of this command you should see server's certificate:
        <pre>
        -----BEGIN CERTIFICATE-----
        {variable sized random bytes}
        -----END CERTIFICATE-----
        subject=/C=US/ST=CA/L=Santa Clara/O=org/OU=org/CN=Sriharsha Chintalapani
        issuer=/C=US/ST=CA/L=Santa Clara/O=org/OU=org/CN=kafka/emailAddress=test@test.com
            </pre>
        If the certificate does not show up or if there are any other error messages than your keystore is not setup properly.</li>

    <li><h4><a id="security_configclients" href="#security_configclients">Configuring Kafka Clients</a></h4>h4>
        SSL is supported only for new Kafka Producer & Consumer, the older API is not supported. The configs for SSL will be same for both producer & consumer.<br>
        If client authentication is not required in the broker, then the following is a minimal configuration example:
        <pre>
        security.protocol = SSL
        ssl.truststore.location = "/var/private/ssl/kafka.client.truststore.jks"
        ssl.truststore.password = "test1234"
            </pre>

        If client authentication is required, then a keystore must be created like in step 1 and the following must also be configured:
            <pre>
        ssl.keystore.location = "/var/private/ssl/kafka.client.keystore.jks"
        ssl.keystore.password = "test1234"
        ssl.key.password = "test1234"
                </pre>
        Other configuration settings that may also be needed depending on our requirements and the broker configuration:\
            <ol>
                <li>ssl.provider (Optional). The name of the security provider used for SSL connections. Default value is the default security provider of the JVM.</li>
                <li>ssl.cipher.suites (Optional). A cipher suite is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol.</li>
                <li>ssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1 **Should list at least one of the protocols configured on the broker side</li>
                <li>ssl.truststore.type = "JKS"</li>
                <li>ssl.keystore.type = "JKS"</li>
            </ol>
<br>
        Examples using console-producer and console-consumer:
        <pre>
            kafka-console-producer.sh --broker-list localhost:9093 --topic test --new-producer --producer-property "security.protocol=SSL"  --producer-property "ssl.truststore.location=client.truststore.jks" --producer-property "ssl.truststore.password=test1234"

            kafka-console-consumer.sh --bootstrap-server localhost:9093 --topic test --new-consumer --consumer.config client-ssl.properties
            </pre>
    </li>
</ol>
<h3><a id="security_sasl" href="#security_sasl">7.3 Authentication using SASL</a></h3>

<ol>
    <li><h4><a id="security_sasl_prereq" href="#security_sasl_prereq">Prerequisites</a></h4><br>
    <ol>
        <li><b>Kerberos</b><br>
        If your organization is already using a Kerberos server (for example, by using Active Directory), there is no need to install a new server just for Kafka. Otherwise you will need to install one, your Linux vendor likely has packages for Kerberos and a short guide on how to install and configure it (<a href="https://help.ubuntu.com/community/Kerberos">Ubuntu</a>, <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Managing_Smart_Cards/installing-kerberos.html">Redhat</a>). Note that if you are using Oracle Java, you will need to download JCE policy files for your Java version and copy them to $JAVA_HOME/jre/lib/security.</li>
        <li><b>Create Kerberos Principals</b><br>
        If you are using the organization's Kerberos or Active Directory server, ask your Kerberos administrator for a principal for each Kafka broker in your cluster and for every Linux user that will access Kafka with Kerberos authentication.</br>
        If you installed your own Kerberos, you will need to create these principals yourself:</br>
            <code>sudo /usr/sbin/kadmin.local -q 'addprinc -randkey kafka/hostname@domainname'<br>
                sudo /usr/sbin/kadmin.local -q "ktadd -k /etc/security/keytabs/kafka.keytab kafka/hostname@domainname"</code></li>
        <li><b>Make sure all hosts can be reachable using hostnames</b> - It is important in case of kerberos all your hosts can be resolved with their FQDNs.</li>
        <li><b><a name="jaas_config_file">Creating JAAS Config File</a></b><br>
            Each node in the cluster should have a JAAS file similar to the example below. Add this file to kafka/config dir:
        <pre>
            KafkaServer {
                com.sun.security.auth.module.Krb5LoginModule required
                useKeyTab=true
                storeKey=true
                serviceName="kafka"
                keyTab="/etc/security/keytabs/kafka1.keytab"
                principal="kafka/kafka1.hostname.com@DOMAIN.COM";
            };

            Client {
               com.sun.security.auth.module.Krb5LoginModule required
               useKeyTab=true
               storeKey=true
               serviceName="zookeeper"
               keyTab="/etc/security/keytabs/kafka1.keytab"
               principal="kafka/kafka1.hostname.com@DOMAIN.COM";
            };

            KafkaClient {
               com.sun.security.auth.module.Krb5LoginModule required
               useTicketCache=true
               serviceName="kafka";
            };
        </pre>
            <u>Important notes:</u>
            <ol>
                <li>KafkaServer is a section name in JAAS file used by KafkaServer/Broker. This section tells Kafka Server which principal to use and which keytab this principal is stored. It allows Kafka Server to login using the keytab specified in this section.</li>
                <li>Client section is used to authenticate a SASL connection with zookeeper. It also allows a broker to set SASL ACL on zookeeper nodes which locks these nodes down so that only kafka broker can modify. It is necessary to have the same principal name across all the brokers. If you want to use a section name other than Client, then you need to set the system property <tt>zookeeper.sasl.client</tt> to the appropriate name (<i>e.g.</i>, <tt>-Dzookeeper.sasl.client=ZkClient</tt>).</li>
                <li>KafkaClient section here describes how the clients like producer and consumer can connect to the Kafka Broker. Here we specified "useTicketCache=true" not a keytab this allows user to do kinit and run a kafka-console-consumer or kafka-console-producer to connect to broker. For a long running process one should create KafkaClient section similar to KafkaServer.</li>
                <li>In KafkaServer and KafkaClient sections we've "serviceName" this should match principal name with which kafka broker is running. In the above example principal="kafka/kafka1.hostname.com@DOMAIN.com" so we've "kafka" which is matching the principalName.</li>
            </ol>
        </li>
        <li><h4><a id="security_sasl_jaas" href="#security_sasl_jaas">Creating Client Side JAAS Config</a></h4>
        Clients (producers, consumers, connect workers, etc) will authenticate to the cluster with their own principal (usually with the same name as the user used for running the client), so obtain or create these principals as needed. Then create a JAAS file as follows:
            <pre>
                KafkaClient {
                    com.sun.security.auth.module.Krb5LoginModule required
                    useKeyTab=true
                    storeKey=true
                    serviceName="kafka"
                    keyTab="/etc/security/keytabs/kafka1.keytab"
                    principal="kafkaproducer/hostname@DOMAIN.COM";
                };
            </pre>
        </li>
    </ol></li>
    <li><h4><a id="security_sasl_brokerconfig" href="#security_sasl_brokerconfig">Configuring Kafka Brokers</a></h4>
    <ol>
        <li>Pass the name of the jaas file you created in <a href="#jaas_config_file">Creating JAAS Config File"</a> as a JVM parameter to the kafka broker: <pre>-Djava.security.auth.login.config=/etc/kafka/kafka_jaas.conf</pre></li>
        <li>Make sure the keytabs configured in the kafka_jaas.conf are readable by the linux user who is starting kafka broker.</li>
        <li>Configure a SASL port in server.properties, by adding the following to the <i>listeners</i> parameter, which contains one or more comma-separated values:
            <pre>listeners=SASL_PLAINTEXT://host.name:port</pre>
        If you are only configuring SASL port (or if you are very paranoid and want the Kafka brokers to authenticate each other using SASL) then make sure you set same SASL protocol for inter-broker communication:
        <pre>security.inter.broker.protocol=SASL_PLAINTEXT</pre></li>

    </ol>
    </li>
    <li><h4><a id="security_sasl_clientconfig" href="#security_sasl_clientconfig">Configuring Kafka Clients</a></h4>
        SASL authentication is only supported for new kafka producer and consumer, the older API is not supported.>br>
        To configure SASL authentication on the clients:
        <ol>
            <li>pass the name of the jaas file you created in <a href="#security_sasl_jaas">Creating Client Side JAAS Config"</a> as a JVM parameter to the client JVM:
        <pre>-Djava.security.auth.login.config=/etc/kafka/kafka_client_jaas.conf</pre></li>
            <li>Make sure the keytabs configured in the kafka_client_jaas.conf are readable by the linux user who is starting kafka client.</li>
            <li>Configure the following property in producer.properties or consumer.properties:
                <pre>security.protocol=SASL_PLAINTEXT</pre></li>
        </ol></li>
</ol>

<h3><a id="security_authz" href="#security_authz">7.4 Authorization and ACLs</a></h3>
Kafka ships with a pluggable Authorizer and an out-of-box authorizer implementation that uses zookeeper to store all the acls. Kafka acls are defined in the general format of "Principal P is [Allowed/Denied] Operation O From Host H On Resource R". You can read more about the acl structure on KIP-11. In order to add, remove or list acls you can use the Kafka authorizer CLI. By default, if a Resource R has no associated acls, no one other than super users is allowed to access R. If you want change that behavior, you can include the following in broker.properties.
<pre>allow.everyone.if.no.acl.found=true</pre>
One can also add super users in broker.properties like the following.
<pre>super.users=User:Bob;User:Alice</pre>
By default, the SSL user name will be of the form "CN=writeuser,OU=Unknown,O=Unknown,L=Unknown,ST=Unknown,C=Unknown". One can change that by setting a customized PrincipalBuilder in broker.properties like the following.
<pre>principal.builder.classs=CustomizedPrincipalBuilderClass</pre>
By default, the SASL user name will be the primary part of the Kerberos principal. One can change that by setting <code>sasl.kerberos.principal.to.local.rules</code> to a customized rule in broker.properties.
<h4><a id="security_authz_cli" href="#security_authz_cli">Command Line Interface</a></h4>
Kafka Authorization management CLI can be found under bin directory with all the other CLIs. The CLI script is called <b>kafka-acls.sh</b>. Following lists all the options that the script supports:
<p></p>
<table class="data-table">
    <tr>
        <th>Option</th>
        <th>Description</th>
        <th>Default</th>
        <th>Option type</th>
    </tr>
    <tr>
        <td>--add</td>
        <td>Indicates to the script that user is trying to add an acl.</td>
        <td></td>
        <td>Action</td>
    </tr>
    <tr>
        <td>--remove</td>
        <td>Indicates to the script that user is trying to remove an acl.</td>
        <td></td>
        <td>Action</td>
    </tr>
    <tr>
        <td>--list</td>
        <td>Indicates to the script that user is trying to list acls.</td>
        <td></td>
        <td>Action</td>
    </tr>
    <tr>
        <td>--authorizer</td>
        <td>Fully qualified class name of the authorizer.</td>
        <td>kafka.security.auth.SimpleAclAuthorizer</td>
        <td>Configuration</td>
    </tr>
    <tr>
        <td>--authorizer-properties</td>
        <td>key=val pairs that will be passed to authorizer for initialization. For the default authorizer the example values are: zookeeper.connect=localhost:2181</td>
        <td></td>
        <td>Configuration</td>
    </tr>
    <tr>
        <td>--cluster</td>
        <td>Specifies cluster as resource.</td>
        <td></td>
        <td>Resource</td>
    </tr>
    <tr>
        <td>--topic [topic-name]</td>
        <td>Specifies the topic as resource.</td>
        <td></td>
        <td>Resource</td>
    </tr>
    <tr>
        <td>--consumer-group [group-name]</td>
        <td>Specifies the consumer-group as resource.</td>
        <td></td>
        <td>Resource</td>
    </tr>
    <tr>
        <td>--allow-principal</td>
        <td>Principal is in PrincipalType:name format that will be added to ACL with Allow permission. <br>You can specify multiple --allow-principal in a single command.</td>
        <td></td>
        <td>Principal</td>
    </tr>
    <tr>
        <td>--deny-principal</td>
        <td>Principal is in PrincipalType:name format that will be added to ACL with Deny permission. <br>You can specify multiple --deny-principal in a single command.</td>
        <td></td>
        <td>Principal</td>
    </tr>
    <tr>
        <td>--allow-hosts</td>
        <td>Comma separated list of hosts from which principals listed in --allow-principals will have access.</td>
        <td> if --allow-principals is specified defaults to * which translates to "all hosts"</td>
        <td>Host</td>
    </tr>
    <tr>
        <td>--deny-hosts</td>
        <td>Comma separated list of hosts from which principals listed in --deny-principals will be denied access.</td>
        <td>if --deny-principals is specified defaults to * which translates to "all hosts"</td>
        <td>Host</td>
    </tr>
    <tr>
        <td>--operations</td>
        <td>Comma separated list of operations.<br>
            Valid values are : Read, Write, Create, Delete, Alter, Describe, ClusterAction, All</td>
        <td>All</td>
        <td>Operation</td>
    </tr>
    <tr>
        <td>--producer</td>
        <td> Convenience option to add/remove acls for producer role. This will generate acls that allows WRITE,
            DESCRIBE on topic and CREATE on cluster.</td>
        <td></td>
        <td>Convenience</td>
    </tr>
    <tr>
        <td>--consumer</td>
        <td> Convenience option to add/remove acls for consumer role. This will generate acls that allows READ,
            DESCRIBE on topic and READ on consumer-group.</td>
        <td>Convenience</td>
    </tr>
</tbody></table>

<h4><a id="security_authz_examples" href="#security_authz_examples">Examples</a></h4>
<ul>
    <li><b>Adding Acls</b><br>
Suppose you want to add an acl "Principals User:Bob and User:Alice are allowed to perform Operation Read and Write on Topic Test-Topic from Host1 and Host2". You can do that by executing the CLI with following options:
        <pre>bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2181 --add --allow-principal User:Bob --allow-principal User:Alice --allow-hosts Host1,Host2 --operations Read,Write --topic Test-topic</pre>
        By default all principals that don't have an explicit acl that allows access for an operation to a resource are denied. In rare cases where an allow acl is defined that allows access to all but some principal we will have to use the --deny-principals and --deny-host option. For example, if we want to allow all users to Read from Test-topic but only deny User:BadBob from host bad-host we can do so using following commands:
        <pre>bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2181 --add --allow-principal User:* --allow-hosts * --deny-principal User:BadBob --deny-hosts bad-host --operations Read--topic Test-topic</pre>
        Above examples add acls to a topic by specifying --topic [topic-name] as the resource option. Similarly user can add acls to cluster by specifying --cluster and to a consumer group by specifying --consumer-group [group-name].</li>

    <li><b>Removing Acls</b><br>
            Removing acls is pretty much the same. The only difference is instead of --add option users will have to specify --remove option. To remove the acls added by the first example above we can execute the CLI with following options:
           <pre> bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2181 --remove --allow-principal User:Bob --allow-principal User:Alice --allow-hosts Host1,Host2 --operations Read,Write --topic Test-topic </pre></li>

    <li><b>List Acls</b><br>
            We can list acls for any resource by specifying the --list option with the resource. To list all acls for Test-topic we can execute the CLI with following options:
            <pre>bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2181 --list --topic Test-topic</pre></li>

    <li><b>Adding or removing a principal as producer or consumer</b><br>
            The most common use case for acl management are adding/removing a principal as producer or consumer so we added convenience options to handle these cases. In order to add User:Bob as a producer of  Test-topic we can execute the following command:
           <pre> bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2181 --add --allow-principal User:Bob --producer --topic Test-topic</pre>
            Similarly to add Alice as a consumer of Test-topic with consumer group Group-1 we just have to pass --consumer option:
           <pre> bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=localhost:2181 --add --allow-principal User:Bob --consumer --topic test-topic --consumer-group Group-1 </pre>
            Note that for consumer option we must also specify the consumer group.
            In order to remove a principal from producer or consumer role we just need to pass --remove option. </li>
    </ul>

<h3><a id="zk_authz" href="#zk_authz">7.5 ZooKeeper Authentication</a></h3>
<h4><a id="zk_authz_new" href="#zk_authz_new">7.5.1 New clusters</a></h4>
To enable ZooKeeper authentication on brokers, there are two necessary steps:
<ol>
	<li> Create a JAAS login file and set the appropriate system property to point to it as described above</li>
	<li> Set the configuration property <tt>zookeeper.set.acl</tt> in each broker to true</li>
</ol>

The metadata stored in ZooKeeper is such that only brokers will be able to modify the corresponding znodes, but znodes are world readable. The rationale behind this decision is that the data stored in ZooKeeper is not sensitive, but inappropriate manipulation of znodes can cause cluster disruption.

<h4><a id="zk_authz_migration" href="#zk_authz_migration">7.5.2 Migrating clusters</a></h4>
If you are running a version of Kafka that does not support security of simply with security disabled, and you want to make the cluster secure, then you need to execute the following steps to enable ZooKeeper authentication with minimal disruption to your operations:
<ol>
	<li>Perform a rolling restart setting the JAAS login file, which enables brokers to authenticate. At the end of the rolling restart, brokers are able to manipulate znodes with strict ACLs, but they will not create znodes with those ACLs</li>
	<li>Perform a second rolling restart of brokers, this time setting the configuration parameter <tt>zookeeper.set.acl</tt> to true, which enables the use of secure ACLs when creating znodes</li>
	<li>Execute the ZkSecurityMigrator tool. To execute the tool, there is this script: <tt>./bin/zookeeper-security-migration.sh</tt> with <tt>zookeeper.acl</tt> set to secure. This tool traverses the corresponding sub-trees changing the ACLs of the znodes</li>
</ol>
<p>It is also possible to turn off authentication in a secure cluster. To do it, follow these steps:</p>
<ol>
	<li>Perform a rolling restart of brokers setting the JAAS login file, which enables brokers to authenticate, but setting <tt>zookeeper.set.acl</tt> to false. At the end of the rolling restart, brokers stop creating znodes with secure ACLs, but are still able to authenticate and manipulate all znodes</li>
	<li>Execute the ZkSecurityMigrator tool. To execute the tool, run this script <tt>./bin/zookeeper-security-migration.sh</tt> with <tt>zookeeper.acl</tt> set to unsecure. This tool traverses the corresponding sub-trees changing the ACLs of the znodes</li>
	<li>Perform a second rolling restart of brokers, this time omitting the system property that sets the JAAS login file</li>
</ol>
Here is an example of how to run the migration tool:
<pre>
./bin/zookeeper-security-migration --zookeeper.acl=secure --zookeeper.connection=localhost:2181
</pre>
<p>Run this to see the full list of parameters:</p>
<pre>
./bin/zookeeper-security-migration --help
</pre>
<h4><a id="zk_authz_ensemble" href="#zk_authz_ensemble">7.5.3 Migrating the ZooKeeper ensemble</a></h4>
It is also necessary to enable authentication on the ZooKeeper ensemble. To do it, we need to perform a rolling restart of the server and set a few properties. Please refer to the ZooKeeper documentation for more detail:
<ol>
	<li><a href="http://zookeeper.apache.org/doc/r3.4.6/zookeeperProgrammers.html#sc_ZooKeeperAccessControl">Apache ZooKeeper documentation</a></li>
	<li><a href="https://cwiki.apache.org/confluence/display/ZOOKEEPER/Zookeeper+and+SASL">Apache ZooKeeper wiki</a></li>
</ol>
